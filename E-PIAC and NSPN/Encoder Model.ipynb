{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xlsxwriter\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "import skimage, os\n",
    "import torch_geometric.nn as geo\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import normalize\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from collections import defaultdict as ddict\n",
    "from torch.utils.data import DataLoader\n",
    "#from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from fast_pytorch_kmeans import KMeans\n",
    "from torchsummary import summary\n",
    "from torch.nn import Linear, ReLU, Dropout\n",
    "from torch_geometric.nn import Sequential, GCNConv, JumpingKnowledge\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch import tensor\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.utils import normalize\n",
    "from skimage.color import rgb2gray\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.init import xavier_uniform_,xavier_normal_,kaiming_uniform_,kaiming_normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9ec49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Loading\n",
    "\n",
    "Image_directory='C:/Users/modak/lung cancer/Noduledetection/data2/'\n",
    "No_nodule=os.listdir(Image_directory+'train_patches_nonodule/')\n",
    "Nodule=os.listdir(Image_directory+'train_patches_nodule/')\n",
    "test_No_nodule=os.listdir(Image_directory+'test_patches_nonodule/')\n",
    "test_Nodule=os.listdir(Image_directory+'test_patches_nodule/')\n",
    "dataset=[]\n",
    "label=[]\n",
    "edge_index=[]\n",
    "graphs=[]\n",
    "data = ddict(list)\n",
    "for i, image_name in enumerate(No_nodule):\n",
    "\n",
    "    if(image_name.split('.')[1]=='png'):\n",
    "        image=cv2.imread(Image_directory+'train_patches_nonodule/'+image_name)\n",
    "        image=Image.fromarray(image, 'RGB')\n",
    "        edges=np.loadtxt(Image_directory+'train_edges_nonodule/'+image_name.split('.')[0]+'edges.txt',dtype=int)\n",
    "        features=np.loadtxt(Image_directory+'train_features_nonodule/'+image_name.split('.')[0]+'feat.txt',dtype=int)\n",
    "        locations=np.loadtxt(Image_directory+'train_location_nonodule/'+image_name.split('.')[0]+'loc.txt',dtype=int)\n",
    "        dataset.append(np.array(image))\n",
    "        edge_index.append(np.array(edges))\n",
    "        data['train'].append({'image':np.array(image),'features':np.mean(np.array(features),axis=1), 'edge_index':np.array(edges.T), 'locations' :np.array(locations), 'label':0})\n",
    "        \n",
    "\n",
    "for i, image_name in enumerate(Nodule):\n",
    "    if(image_name.split('.')[1]=='png'):\n",
    "        image=cv2.imread(Image_directory+'train_patches_nodule/'+image_name)\n",
    "        image=Image.fromarray(image, 'RGB')\n",
    "        edges=np.loadtxt(Image_directory+'train_edges_nodule/'+image_name.split('.')[0]+'edges.txt',dtype=int)\n",
    "        features=np.loadtxt(Image_directory+'train_features_nodule/'+image_name.split('.')[0]+'feat.txt',dtype=int)\n",
    "        locations=np.loadtxt(Image_directory+'train_location_nodule/'+image_name.split('.')[0]+'loc.txt',dtype=int)\n",
    "        dataset.append(np.array(image))\n",
    "        edge_index.append(np.array(edges))\n",
    "        data['train'].append({'image':np.array(image),'features':np.mean(np.array(features),axis=1), 'edge_index':np.array(edges.T), 'locations' :np.array(locations), 'label':1})\n",
    "        \n",
    "        \n",
    "for i, image_name in enumerate(test_No_nodule):\n",
    "\n",
    "    if(image_name.split('.')[1]=='png'):\n",
    "        image=cv2.imread(Image_directory+'test_patches_nonodule/'+image_name)\n",
    "        image=Image.fromarray(image, 'RGB')\n",
    "        edges=np.loadtxt(Image_directory+'test_edges_nonodule/'+image_name.split('.')[0]+'edges.txt',dtype=int)\n",
    "        features=np.loadtxt(Image_directory+'test_features_nonodule/'+image_name.split('.')[0]+'feat.txt',dtype=int)\n",
    "        locations=np.loadtxt(Image_directory+'test_location_nonodule/'+image_name.split('.')[0]+'loc.txt',dtype=int)\n",
    "        #image=image.resize((256,256))\n",
    "        dataset.append(np.array(image))\n",
    "        edge_index.append(np.array(edges))\n",
    "        data['test'].append({'image':np.array(image),'features':np.mean(np.array(features),axis=1), 'edge_index':np.array(edges.T), 'locations' :np.array(locations), 'label':0})\n",
    "        \n",
    "\n",
    "for i, image_name in enumerate(test_Nodule):\n",
    "    if(image_name.split('.')[1]=='png'):\n",
    "        image=cv2.imread(Image_directory+'test_patches_nodule/'+image_name)\n",
    "        image=Image.fromarray(image, 'RGB')\n",
    "        edges=np.loadtxt(Image_directory+'test_edges_nodule/'+image_name.split('.')[0]+'edges.txt',dtype=int)\n",
    "        features=np.loadtxt(Image_directory+'test_features_nodule/'+image_name.split('.')[0]+'feat.txt',dtype=int)\n",
    "        locations=np.loadtxt(Image_directory+'test_location_nodule/'+image_name.split('.')[0]+'loc.txt',dtype=int)\n",
    "        dataset.append(np.array(image))\n",
    "        edge_index.append(np.array(edges))\n",
    "        data['test'].append({'image':np.array(image),'features':np.mean(np.array(features),axis=1), 'edge_index':np.array(edges.T), 'locations' :np.array(locations), 'label':1})\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e53e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.set_rng_state(torch.cuda.get_rng_state())\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a45b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(split):\n",
    "    trainloader=DataLoader(data[split],\n",
    "                batch_size=256,\n",
    "                shuffle=True,# if split=='train' else False,\n",
    "                num_workers=0,\n",
    "                drop_last=True \n",
    "                \n",
    "                )\n",
    "    return trainloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ef4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_param(shape):\n",
    "    param = Parameter(torch.Tensor(*shape));\n",
    "    #kaiming_uniform_(param.data, a=0, mode='fan_out', nonlinearity='leaky_relu')\n",
    "    \n",
    "    kaiming_uniform_(param.data)\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NSPN Deployment\n",
    "from Conv_new import NSPN\n",
    "class superpixelnetwork(torch.nn.Module):\n",
    "    def __init__(self, input_features,hidden_channels):\n",
    "        super(superpixelnetwork, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        self.numnodes=256\n",
    "        self.input_features=input_features\n",
    "        self.hidden_channels=hidden_channels\n",
    "        self.bn0 = torch.nn.BatchNorm2d(3)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(3)\n",
    "\n",
    "        self.conv1 = NSPN(self.input_features, int(self.hidden_channels/2),self.numnodes)\n",
    "        self.conv2 = NSPN(int(self.hidden_channels/2),self.hidden_channels,self.numnodes)\n",
    "        self.weighte= get_param((3,32, 32))\n",
    "        self.weightg= get_param((3,16,16))\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(4,4)\n",
    "        self.pool0 = nn.MaxPool2d(8,8)\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(7936,128 ,bias=True) \n",
    "        self.fc2 = torch.nn.Linear(128,1 ,bias=True) \n",
    "\n",
    "\n",
    "    def forward(self, x,image,locs, edge_index):\n",
    "        \n",
    "        image=image.view(-1,3,128,128)/256\n",
    "        \n",
    "        \n",
    "        x=x/256\n",
    "        x=x.view(-1,x.shape[1],1)\n",
    "        \n",
    "        \n",
    "        for i in range(0,x.shape[0]):\n",
    "            \n",
    "            \n",
    "            y = self.conv1.forward(x=x[i], edge_index=edge_index[i].type(torch.int64))\n",
    "           \n",
    "            y=self.conv2.forward(x=y, edge_index=edge_index[i].type(torch.int64))\n",
    "\n",
    "            y=F.relu(y)\n",
    "            \n",
    "\n",
    "            if i==0:\n",
    "                z1=y\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                z1=torch.cat((z1,y),0)\n",
    "                \n",
    "        \n",
    "        \n",
    "        z1=z1.view(-1,1,self.numnodes,self.hidden_channels)\n",
    "\n",
    "        z1=z1.view(-1,z1.shape[1]*z1.shape[2]*z1.shape[3])\n",
    "        \n",
    "        \n",
    "        \n",
    "        z2=self.pool1(image)\n",
    "        \n",
    "        z2=torch.matmul(z2,self.weighte)\n",
    "        z2=self.bn1(z2)\n",
    "        z2=F.relu(z2)\n",
    "        \n",
    "        z0=self.pool0(image)\n",
    "        \n",
    "        z0=torch.matmul(z0,self.weightg)\n",
    "        z0=self.bn0(z0)\n",
    "        z0=F.relu(z0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        z2=z2.view(-1,z2.shape[1]*z2.shape[2]*z2.shape[3])\n",
    "        z0=z0.view(-1,z0.shape[1]*z0.shape[2]*z0.shape[3])\n",
    "        \n",
    "        z=self.fc1(torch.cat((z0,z1,z2),1))\n",
    "        \n",
    "        \n",
    "  \n",
    "        z=self.fc2(z)\n",
    "\n",
    "        z = torch.sigmoid(z)\n",
    "        #print(z)\n",
    "        return z\n",
    "\n",
    "model = superpixelnetwork(input_features=1,hidden_channels=16)\n",
    "model.double()\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model.to(device)\n",
    "print(model)\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a40e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training \n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.00001, weight_decay=0.001)\n",
    "loss_vec_train=[]\n",
    "loss_vec_test=[]\n",
    "epc=[]\n",
    "acc=[]\n",
    "acc2=[]\n",
    "losterm=nn.BCELoss()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correction=[]\n",
    "    correction2=[]\n",
    "    losses2=[]\n",
    "    label_prime=[]\n",
    "    predicted_prime=[]\n",
    "    roc_test=[]  \n",
    "    for batch in dataloader('train'):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward((batch['features']).to(device),(batch['image'].double()).to(device),(batch['locations'].double()).to(device), (batch['edge_index']).to(device))\n",
    "        \n",
    "\n",
    "        label=torch.tensor(batch['label'],dtype=torch.double,requires_grad=False)\n",
    "        #print(label.shape)\n",
    "        output=output.view(-1)\n",
    "\n",
    "        loss = F.binary_cross_entropy(output, label.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append((loss.cpu()).detach().numpy())\n",
    "         \n",
    "        pred = torch.round(output)\n",
    "\n",
    "\n",
    "        correct = int((pred.cpu() == batch['label']).sum())\n",
    "        correction.append(correct)\n",
    "        \n",
    "        \n",
    "    correction=np.array(correction)\n",
    "    accuracy=np.sum(correction)/len(data['train'])\n",
    "    losses=np.mean(losses)\n",
    "    \n",
    "    model.eval()\n",
    "    for batch2 in dataloader('test'):\n",
    "        \n",
    "        evaluate=model.forward((batch2['features']).to(device),(batch2['image'].double()).to(device),(batch2['locations'].double()).to(device), (batch2['edge_index']).to(device))\n",
    "        \n",
    "        label2=torch.tensor(batch2['label'],dtype=torch.double,requires_grad=False)\n",
    "        evaluate=evaluate.view(-1)\n",
    "        \n",
    "        loss2 = F.binary_cross_entropy(evaluate, label2.to(device))\n",
    "        label_prime.append((label2.cpu()).detach().numpy())\n",
    "        predicted_prime.append((evaluate.cpu()).detach().numpy())\n",
    "        pred2 = torch.round(evaluate)\n",
    "     \n",
    "        correct2 = int((pred2.cpu() == batch2['label']).sum())\n",
    "        correction2.append(correct2)\n",
    "        losses2.append((loss2.cpu()).detach().numpy())\n",
    "    predicted_prime=np.array(predicted_prime)\n",
    "    label_prime=np.array(label_prime)\n",
    "    predicted_prime=np.array(np.reshape(predicted_prime,(predicted_prime.shape[0]*predicted_prime.shape[1],-1)))\n",
    "    \n",
    "    label_prime=np.array(np.reshape(label_prime,(label_prime.shape[0]*label_prime.shape[1],-1)))\n",
    "    if epoch==49:\n",
    "        lr_fpr, lr_tpr, _ =roc_curve(label_prime[:, 0], predicted_prime[:, 0])\n",
    "\n",
    "    losses2=np.mean(losses2)\n",
    "    correction2=np.array(correction2)\n",
    "    accuracy2=np.sum(correction2)/len(data['test'])\n",
    "    print('train accuracy:',round(accuracy,4),'test accuracy:',round(accuracy2,4),'train loss:',round(losses,4),'test loss:', round(losses2,4))\n",
    "    loss_vec_train.append(losses)\n",
    "    loss_vec_test.append(losses2)\n",
    "    epc.append(epoch)\n",
    "    acc.append(accuracy)\n",
    "    acc2.append(accuracy2)\n",
    "\n",
    "loss_vec_train=np.array(loss_vec_train)\n",
    "loss_vec_test=np.array(loss_vec_test)\n",
    "acc=np.array(acc)\n",
    "acc2=np.array(acc2)\n",
    "epc=np.array(epc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'C:/Users/modak/lung cancer/Noduledetection/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snakes",
   "language": "python",
   "name": "snakes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
